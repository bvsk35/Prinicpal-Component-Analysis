{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "#### Introduction \n",
    "To extract prinicipal components Hebbian learning rule was modified by Erkki Oja and developed Oja's rule which is used to extract first prinicipal component (i.e. eigen vector corresponding to largest eigen value). Generalized Hebbian Algorithm (GHA) can be used to all prinicipal components. The Generalized Hebbian Algorithm (GHA), is also known in the literature as Sanger's rule, is a linear feedforward neural network model for unsupervised learning. GHA combines Oja's rule with the Gram-Schmidt process to produce a learning rule. Please refer to following links: https://en.wikipedia.org/wiki/Oja%27s_rule, https://en.wikipedia.org/wiki/Generalized_Hebbian_Algorithm. Many more links on this topic can be found in google. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GHA\n",
    "Two kind of datasets are here one of the shape \"X\" and another which is just a diagonal line. Here we develop a single layer feedforward neural network with linear activation function and no bias. For this example since our dataset is two dimensional and we want to extract all the PCs we will have two outputs in the network. \n",
    "\n",
    "Note: result for the former dataset can reach a local solution ($[0.707, 0.707]$ and $[-0.707, 0.707]$) instead of global sol ($[1, 0]$ and $[0, 1]$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PCA using GHA Extracting All PCs\n",
    "\n",
    "# Parameters\n",
    "# X = numpy.array([[1, 1], [1, -1], [-1, 1], [-1, -1], [2, 2], [2, -2], [-2, 2], [-2, -2]])\n",
    "X = numpy.array([[-1, -1], [0, 0], [1, 1], [-2, -2], [2, 2], [-1.1, -0.8], [-2.1, -1.8], [1.1, 0.8], [2.1, 1.8]])\n",
    "wig = numpy.random.normal(0, 0.5, (2, 2))\n",
    "wig_Norm = wig/numpy.linalg.norm(wig, axis=1).reshape(2, 1)\n",
    "eta = 0.2\n",
    "epoch = 1\n",
    "max_epoch = 200000\n",
    "\n",
    "# Required Functions\n",
    "def update_weights(lr, x, W, iterations):\n",
    "    y = numpy.dot(W, x)\n",
    "    LT = numpy.tril(numpy.matmul(y[:, numpy.newaxis], y[numpy.newaxis, :]))\n",
    "    W = W + lr/iterations * ((y[:, numpy.newaxis] * x) - (numpy.matmul(LT, W)))\n",
    "    return W\n",
    "    \n",
    "# Main\n",
    "W_new = deepcopy(wig_Norm)\n",
    "while epoch <= max_epoch:\n",
    "    for i in range(0, 8):\n",
    "        W_new = update_weights(eta, X[i], W_new, epoch)\n",
    "#     print('Epoch: ', epoch, ' LR: ', eta)\n",
    "    epoch += 1\n",
    "print('Optimal Weights Reached!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular Value Decompostion\n",
    "Analytically solution can be found for PCA using SVD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD\n",
    "Mean = numpy.mean(X, axis=0)\n",
    "X_Norm = X - Mean\n",
    "row, col = X.shape\n",
    "Sample_Cov_Matrix = numpy.matmul(X_Norm.T, X_Norm)/(row - 1)\n",
    "Eigen_Values, Eigen_Vectors = numpy.linalg.eig(Sample_Cov_Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "Solution using SVD and GHA can be compared if they are close or not. GHA gives all the principal components and we can take dot product between them to see if they are orthogonal or not. To see if SVD and GHA solution are close we can take cross product between PC's found using GHA and SVD. If they are close then cross product should yield answer as zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "print('Initial Guess:')\n",
    "print(wig)\n",
    "print(wig_Norm)\n",
    "print('Final Sol:')\n",
    "print(W_new)\n",
    "print(W_new/numpy.linalg.norm(W_new, axis=1).reshape(2, 1))\n",
    "print('Check if GHA 2 PCs are orthogonal: ')\n",
    "print(numpy.dot(W_new[0], W_new[1]))\n",
    "print('SVD and GHA Sol:')\n",
    "print(Eigen_Vectors)\n",
    "print(W_new.T)\n",
    "print('Check if SVD and GHA sol are close:')\n",
    "print(numpy.cross(W_new.T[:, 0], Eigen_Vectors[:, 0]))\n",
    "print(numpy.cross(W_new.T[:, 1], Eigen_Vectors[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "Plot the dataset and PCs found using GHA and SVD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Results\n",
    "plt.plot(X[:, 0], X[:, 1], '.')\n",
    "plt.plot([0, W_new[0, 0]], [0, W_new[0, 1]], 'r')\n",
    "plt.plot([0, W_new[1, 0]], [0, W_new[1, 1]], 'g')\n",
    "plt.plot([0, Eigen_Vectors[0, 0]], [0, Eigen_Vectors[1, 0]], 'k')\n",
    "plt.plot([0, Eigen_Vectors[0, 1]], [0, Eigen_Vectors[1, 1]], 'k')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
