{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Component Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction:\n",
    "PCA was invented in 1901 by Karl Pearson as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s. Depending on the field of application, it is also named the discrete Karhunen–Loève transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (Golub and Van Loan, 1983), eigenvalue decomposition (EVD) of $\\textbf{X}^{T}\\textbf{X}$ in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch. 7 of Jolliffe's Principal Component Analysis), Eckart–Young theorem (Harman, 1960), or empirical orthogonal functions (EOF) in meteorological science, empirical eigenfunction decomposition (Sirovich, 1987), empirical component analysis (Lorenz, 1956), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics. For more details refer https://en.wikipedia.org/wiki/Principal_component_analysis\n",
    "\n",
    "A simple example has been used to demonstrate PCA. Dataset used here is in the shape of 'X'. PCA was done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix, after a normalization step of the initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "# Plot the points\n",
    "Points = numpy.array([[6, 6], [4, 4], [4, 6], [6, 4], [7, 7], [3, 3], [3, 7], [7, 3]])\n",
    "plt.plot(Points[:, 0], Points[:, 1], 'rd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1\n",
    "While doing PCA on any kind of dataset, the first step would be to normalize the data i.e. data should be centered around origin. Hence it will have zero mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the dataset to have zero mean\n",
    "Mean = numpy.mean(Points, axis=0)\n",
    "Points_Norm = Points - Mean\n",
    "plt.plot(Points_Norm[:, 0], Points_Norm[:, 1], 'rd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "Calculate the Sample Covariance Matrix of the normalised data and then calculate the eigen values and eigen vectors for the Sample Covariance Matrix.\n",
    "Brute force way of calculating the sample covarince matrix can be very tough when vector is very large. Easy way to do is to follow the matrix calculations. This has been taken from the following: https://en.wikipedia.org/wiki/Sample_mean_and_covariance.\n",
    "\n",
    "\\begin{align*}\n",
    "    Q = \\frac{1}{N-1} (\\textbf{M} - \\textbf{1}_{\\textbf{N}}\\overline{\\textbf{x}}^{\\textbf{T}})^{T} (\\textbf{M} - \\textbf{1}_{\\textbf{N}}\\overline{\\textbf{x}}^{\\textbf{T}})\n",
    "\\end{align*}\n",
    "Q: Sample Covariance Matrix\n",
    "\n",
    "N: Total No. of Observations\n",
    "\n",
    "M: Column Vector of all observations = $[x_{1} x_{2} x_{3}............. x_{N}]^{T}$. This is a NxK matrix whose column j is the vector of N observations on variable j.\n",
    " \n",
    "$\\overline{\\textbf{x}}$:  is a 1×K row vector, which is mean of each variable on all observations\n",
    "\n",
    "$x_{i}$: is a K dimensional vector, where K is total no. of variables in each vector\n",
    "\n",
    "$\\textbf{1}_{\\textbf{N}}$: is an Nx1 vector of ones\n",
    "\n",
    "Note: Each observation $x_{i}$ is a K dimensional vector ($x_{i} \\in \\mathbb{R}^{K}$). We will use PCA to reduce the dimension say to D i.e. $x_{reduced} \\in \\mathbb{R}^{D}$ $D<<K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row, col = Points_Norm.shape\n",
    "Sample_Cov_Matrix = numpy.matmul(Points_Norm.T, Points_Norm)/(row - 1)\n",
    "Eigen_Values, Eigen_Vectors = numpy.linalg.eig(Sample_Cov_Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "Next arrange all the eigen values in descending order and re-arrange the eigen vector matrix such that first eigen vector (First Principal Component) corresponds to the largest eigen value, second eigen vector (Second Principal Component) corresponds to the next largest eigen value and so on. Let's call this matrix as $U$. We will use the first $D$ columns of the $U$ matrix and call it $U_{reduced}$, this will be the new basis for our system.\n",
    "\n",
    "We can project observation $x_{i}$ into the eigen space/principal space defined by $U_{reduced}$ by computing\n",
    "\n",
    "\\begin{align*}\n",
    "    x_{reduced} = U_{reduced}^{T}x_{i}\n",
    "\\end{align*}\n",
    "\n",
    "this transforms our original point $x_{i}$ in $\\mathbb{R}^{K}$ into a point $x_{reduced}$ in some reduced data space $\\mathbb{R}^{D}$.\n",
    "\n",
    "If we want to do PCA for all points in single step then,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\textbf{X}_{reduced} = U_{reduced}^{T}\\textbf{X}^{T}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\textbf{X}$ is NxK matrix, $\\textbf{X}_{reduced}$ is a DxN matrix, $U_{reduced}$ is KxD matrix, and $U$ is KxK matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = col\n",
    "D = 2 # Since x is 2D vector i.e. K = 2, D can be either 1 or 2\n",
    "Sort_Index = numpy.argsort(Eigen_Values)[::-1]\n",
    "U = Eigen_Vectors[:, Sort_Index]\n",
    "U_reduced = U[:, 0:D]\n",
    "PC_Proj = numpy.matmul(U_reduced.T, Points_Norm.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "We can re-construct/inverse transform the $x_{reduced}$ in eigen space/principal space to $x_{i}$ in original space. We lose some information when we do PCA and hence when we reconstruct we will not get the original vector but something close. We lose data in the compression, but even for fairly small values of $D$, we retain the “essence” of the data. To return to the original data space, we compute:\n",
    "\n",
    "\\begin{align*}\n",
    "    x^{'} = U_{reduced}x_{reduced}\n",
    "\\end{align*}\n",
    "\n",
    "$x^{'}$ is close to $x_{i}$. If we want to do inverse transform for all points in single step then,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\textbf{X}^{'} = \\textbf{X}_{reduced}^{T}U_{reduced}^{T}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\textbf{X}^{'}$ is NxK matrix, $\\textbf{X}_{reduced}$ is DxN matrix and $U_{reduced}$ is KxD matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Points_Prime = numpy.matmul(PC_Proj.T, U_reduced.T)\n",
    "plt.plot(Points_Prime[:, 0], Points_Prime[:, 1], 'gd')\n",
    "plt.plot(Points_Norm[:, 0], Points_Norm[:, 1], 'rd')\n",
    "plt.quiver([0], [0], [3*U[0, 1]], [0], scale_units = 'xy', scale = 1)\n",
    "plt.quiver([0], [0], [0], [3*U[1, 0]], scale_units = 'xy', scale = 1)\n",
    "plt.axis([-4, 4, -4, 4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot showing the cumulative variance.\n",
    "Significance = [numpy.abs(i)/numpy.sum(Eigen_Values) for i in Eigen_Values]\n",
    "x = numpy.arange(1, K+1)\n",
    "y = numpy.cumsum(Significance)\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree Plot showing Variance of each Principal Component\n",
    "plt.plot(x, Significance)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') # for each component\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
